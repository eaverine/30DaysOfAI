{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking objects using color spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to get the current frame from the video capture object\n",
    "def get_frame(cap, scaling_factor):\n",
    "    #Read the current frame from the video capture object\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    #Resize the image\n",
    "    frame = cv2.resize(frame, None, fx = scaling_factor, fy = scaling_factor, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_video_capture():\n",
    "    cap = cv2.VideoCapture('data/video_sugar_the_movie.mp4')\n",
    "    \n",
    "    #Define the scaling factor for the images\n",
    "    scaling_factor = 0.5\n",
    "    \n",
    "    #Keep reading the frames from the video capture object until the user hits the 'Esc' key\n",
    "    while True:\n",
    "        #Grab the current frame\n",
    "        frame = get_frame(cap, scaling_factor)\n",
    "        \n",
    "        #Convert the image to HSV colorspace\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Define range of skin color in HSV\n",
    "        lower = np.array([0, 70, 60])\n",
    "        upper = np.array([50, 150, 255])\n",
    "        \n",
    "        #Threshold the HSV image to only get cyan blue color\n",
    "        mask = cv2.inRange(hsv, lower, upper)\n",
    "        \n",
    "        #Compute the Bitwise-AND between the mask and the original image\n",
    "        img_bitwise_and = cv2.bitwise_and(frame, frame, mask = mask)\n",
    "        \n",
    "        #Run median blurring to smoothen the image\n",
    "        img_median_blurred = cv2.medianBlur(img_bitwise_and, 5)\n",
    "        \n",
    "        #Display the input and output\n",
    "        cv2.imshow('Input', frame)\n",
    "        cv2.imshow('Output', img_median_blurred)\n",
    "        \n",
    "        #Check if the user hits the 'Esc' key\n",
    "        stop_key = cv2.waitKey(5)\n",
    "        if stop_key == 27:\n",
    "            break\n",
    "            \n",
    "    #Closing all windows properly\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_video_capture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object tracking using background subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cv2.createBackgroundSubtractorMOG2()\n",
    "x.apply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_video_capture_bg_subtract():\n",
    "    # Define the video capture object\n",
    "    cap = cv2.VideoCapture('data/video_sugar_the_movie.mp4')\n",
    "    \n",
    "    #Define the background subtractor object\n",
    "    bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "    \n",
    "    # Define the number of previous frames to use to learn. This factor controls the learning rate of the algorithm.\n",
    "    # The learning rate refers to the rate at which your model will learn about the background. \n",
    "    # Higher value for 'history' indicates a slower learning rate.\n",
    "    history = 100\n",
    "    learning_rate = 1/history #Define the learning rate\n",
    "    \n",
    "    scaling_factor = 0.5\n",
    "    \n",
    "    #Keep reading the frames from the video capture object until the user hits the 'Esc' key\n",
    "    while True:\n",
    "        #Grab the current frame\n",
    "        frame = get_frame(cap, scaling_factor)\n",
    "        \n",
    "        #Compute the mask\n",
    "        mask = bg_subtractor.apply(frame, learningRate = learning_rate)\n",
    "        \n",
    "        #Convert the mask from grayscale to rgb\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        #Display the images\n",
    "        cv2.imshow('Input', frame)\n",
    "        cv2.imshow('Output', frame & mask)\n",
    "        \n",
    "        #Check if the user hits the 'Esc' key\n",
    "        stop_key = cv2.waitKey(10)\n",
    "        if stop_key == 27:\n",
    "            break\n",
    "            \n",
    "    #Release the video capture object\n",
    "    cap.release()\n",
    "    #Close all windows\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_video_capture_bg_subtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an interactive object tracker using the CAMShift algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a class to handle object tracking-related functionality\n",
    "class ObjectTracker(object):\n",
    "    def __init__(self, scaling_factor = 0.5):\n",
    "        #Initialize the video capture object\n",
    "        self.cap = cv2.VideoCapture('data/video_messi.mp4')\n",
    "        \n",
    "        #Capture the frame from the video capture object\n",
    "        _, self.frame = self.cap.read()\n",
    "        \n",
    "        self.scaling_factor = scaling_factor #Scaling factor for the captured frame\n",
    "        \n",
    "        #Resize the frame\n",
    "        self.frame = cv2.resize(self.frame, None, fx = self.scaling_factor, fy = self.scaling_factor, \n",
    "                                interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        #Create a window to display the frame\n",
    "        cv2.namedWindow('Object Tracker')\n",
    "        \n",
    "        #Set the mouse callback function to track the mouse\n",
    "        cv2.setMouseCallback('Object Tracker', self.mouse_event)\n",
    "        \n",
    "        #Initialize variables to track the rectangular section for the object to be tracked\n",
    "        self.selection = None   #initialize variable related to rectangular region selection\n",
    "        self.drag_start = None   #Initialize variable related to starting position\n",
    "        self.tracking_state = 0   #Initialize variable related to the state of tracking\n",
    "       \n",
    "    \n",
    "    def mouse_event(self, event, x, y, flags, param):\n",
    "        #Define a method to track mouse events\n",
    "        x, y = np.int16([x, y])   #Convert x and y coordinates into 16-bit numpy integers\n",
    "        \n",
    "        #Check if a mouse button down event has occured since that should indicate the user has started drawing a rectangle\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            self.drag_start = (x, y)\n",
    "            self.tracking_state = 0\n",
    "            \n",
    "        #Check if the user has started dragging the mouse, to select the region\n",
    "        if self.drag_start:\n",
    "            if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "                #Extract the dimensions of the frame\n",
    "                h, w = self.frame.shape[:2]\n",
    "                \n",
    "                #Get the initial position\n",
    "                xi, yi = self.drag_start\n",
    "                \n",
    "                #Get the max and min values to make it agnostic to the direction that the mouse is dragged to\n",
    "                x0, y0 = np.maximum(0, np.minimum([xi, yi], [x, y]))\n",
    "                x1, y1 = np.minimum([w, h], np.maximum([xi, yi], [x, y]))\n",
    "                \n",
    "                #Reset the selection variable\n",
    "                self.selection = None\n",
    "                \n",
    "                #Finalize the rectangular selection\n",
    "                if (x1 - x0 > 0) and (y1 - y0 > 0):\n",
    "                    self.selection = (x0, y0, x1, y1)\n",
    "            \n",
    "            else:\n",
    "                #If the selection is done, start tracking\n",
    "                self.drag_start = None\n",
    "                if self.selection is not None:\n",
    "                    self.tracking_state = 1\n",
    "        \n",
    "    \n",
    "    def start_tracking(self):\n",
    "        #Iterate until the user hits the 'Esc' key\n",
    "        while True:\n",
    "            #Capture the frame from the video capture object\n",
    "            _, self.frame = self.cap.read()\n",
    "            \n",
    "            #Resize the input frame\n",
    "            self.frame = cv2.resize(self.frame, None, fx = self.scaling_factor, fy = self.scaling_factor,\n",
    "                                   interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "            #Create a copy of the frame, which will be used later in the code\n",
    "            vis = self.frame.copy()\n",
    "            \n",
    "            #Convert the frame to HSV Colorspace\n",
    "            hsv = cv2.cvtColor(self.frame, cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            #Create the mask based on predefined thresholds\n",
    "            mask = cv2.inRange(hsv, np.array([0., 60., 32.]), np.array([180., 255., 255.]))\n",
    "            \n",
    "            #Check if the user has selected the region\n",
    "            if self.selection:\n",
    "                #Extract the coordinates of the selected rectangle\n",
    "                x0, y0, x1, y1 = self.selection\n",
    "                \n",
    "                #Extract the tracking window\n",
    "                self.track_window = (x0, y0, x1-x0, y1-y0)\n",
    "                \n",
    "                #Extract the regions of interest\n",
    "                hsv_roi = hsv[y0:y1, x0:x1]\n",
    "                mask_roi = mask[y0:y1, x0:x1]\n",
    "                \n",
    "                #Compute the histogram of the region of interest in the HSV image using the mask\n",
    "                hist = cv2.calcHist([hsv_roi], [0], mask_roi, [16], [0, 180])\n",
    "                \n",
    "                #Normalize and reshape the histogram\n",
    "                cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX)\n",
    "                self.hist = hist.reshape(-1)\n",
    "                \n",
    "                #Extract the region of interest from the frame\n",
    "                vis_roi = vis[y0:y1, x0:x1]\n",
    "                \n",
    "                #Compute the image negative (for display only)\n",
    "                cv2.bitwise_not(vis_roi, vis_roi)\n",
    "                vis[mask == 0] = 0\n",
    "                \n",
    "            #Check if the system is in the 'tracking' mode\n",
    "            if self.tracking_state == 1:\n",
    "                self.selection = None   #Reset the selection variable\n",
    "                \n",
    "                #Compute the histogram back projection\n",
    "                hsv_backproj = cv2.calcBackProject([hsv], [0], self.hist, [0, 180], 1)\n",
    "                \n",
    "                #Compute bitwise AND between histogram\n",
    "                hsv_backproj &= mask\n",
    "                \n",
    "                #Define termination criteria for the tracker\n",
    "                term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "                \n",
    "                #Apply the CAMShift algorithm on the back project histogram\n",
    "                track_box, self.track_window = cv2.CamShift(hsv_backproj, self.track_window, term_crit)\n",
    "                \n",
    "                #Draw an ellipse around the object\n",
    "                cv2.ellipse(vis, track_box, (0, 255, 0), 2)\n",
    "            \n",
    "            #Show the output live video\n",
    "            cv2.imshow('Object Tracker', vis)\n",
    "        \n",
    "            #Stop if the user hits the 'Esc' key\n",
    "            stop_key = cv2.waitKey(5)\n",
    "            if stop_key == 27:\n",
    "                break\n",
    "        \n",
    "        #Close all windows\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObjectTracker().start_tracking()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
