{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical flow-based tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_tracking():\n",
    "    #Define a function to track the object\n",
    "    cap = cv2.VideoCapture('data/startup.mp4')   #Initialize the video capture object\n",
    "    scaling_factor = 0.5   #Define the scaling factor for the frames\n",
    "    \n",
    "    #Number of frames to track\n",
    "    num_frames_to_track = 5\n",
    "    \n",
    "    #Skipping factor\n",
    "    num_frames_jump = 2\n",
    "    \n",
    "    #Initialize variables\n",
    "    tracking_paths = []\n",
    "    frame_index = 0\n",
    "    \n",
    "    #Define tracking parameters\n",
    "    tracking_params = dict(winSize = (11, 11), maxLevel = 2, \n",
    "                           criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "    \n",
    "    #Iterate until the user hits the 'Esc' key\n",
    "    while True:\n",
    "        #Capture the current frame\n",
    "        _, frame = cap.read()\n",
    "        \n",
    "        #Resize the frame\n",
    "        frame = cv2.resize(frame, None, fx = scaling_factor, fy = scaling_factor, interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        #Convert the frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #Create a copy of the frame\n",
    "        output_img = frame.copy()\n",
    "        \n",
    "        #Get images if tracking path remaining is more than 0\n",
    "        if len(tracking_paths) > 0:\n",
    "            prev_img, current_img = prev_gray, frame_gray\n",
    "        \n",
    "            #Organize the feature points\n",
    "            feature_points_0 = np.float32([tp[-1] for tp in tracking_paths]).reshape(-1, 1, 2)\n",
    "        \n",
    "            #Compute the optical flow \n",
    "            feature_points_1, _, _ = cv2.calcOpticalFlowPyrLK(prev_img, current_img, feature_points_0, None, **tracking_params)\n",
    "        \n",
    "            #Compute reverse optical flow\n",
    "            feature_points_0_rev, _, _ = cv2.calcOpticalFlowPyrLK(current_img, prev_img, feature_points_1, None, \n",
    "                                                                  **tracking_params)\n",
    "        \n",
    "            #Compute the difference between the forward and reverse optical flow\n",
    "            diff_feature_points = abs(feature_points_0 - feature_points_0_rev).reshape(-1, 2).max(-1)\n",
    "        \n",
    "            #Extract the good points\n",
    "            good_points = diff_feature_points < 1\n",
    "        \n",
    "            #Initialize variable\n",
    "            new_tracking_paths = []\n",
    "        \n",
    "            #Iterate through all the good feature points and draw circles around them\n",
    "            for tp, (x, y), good_points_flag in zip(tracking_paths, feature_points_1.reshape(-1, 2), good_points):\n",
    "                #If the flag is not true, then continue\n",
    "                if not good_points_flag:\n",
    "                    continue\n",
    "                \n",
    "                #Append the X and Y coordinates and check if the length is greater than the threshold\n",
    "                tp.append((x, y))\n",
    "                if len(tp) > num_frames_to_track:\n",
    "                    del tp[0]\n",
    "                \n",
    "                new_tracking_paths.append(tp)\n",
    "            \n",
    "                #Draw a circle around the feature points\n",
    "                cv2.circle(output_img, (int(x), int(y)), 3, (0, 255, 0), -1)\n",
    "            \n",
    "            #Update the tracking paths\n",
    "            tracking_paths = new_tracking_paths\n",
    "        \n",
    "            #Draw lines using the new tracking paths to show movement\n",
    "            cv2.polylines(output_img, [np.int32(tp) for tp in tracking_paths], False, (0, 150, 0))\n",
    "            \n",
    "        #Go into this after skipping the right number of frames\n",
    "        if not frame_index % num_frames_jump:\n",
    "            #Create a mask and draw the circles\n",
    "            mask = np.zeros_like(frame_gray)\n",
    "            mask[:] = 255\n",
    "            for x, y in [np.int32(tp[-1]) for tp in tracking_paths]:\n",
    "                cv2.circle(mask, (int(x), int(y)), 6, 0, -1)\n",
    "                \n",
    "            #Compute good features to track\n",
    "            feature_points = cv2.goodFeaturesToTrack(frame_gray, mask = mask, maxCorners = 500, qualityLevel = 0.3,\n",
    "                                                    minDistance = 7, blockSize = 7)\n",
    "            #If feature points exist, append them to tracking paths\n",
    "            if feature_points is not None:\n",
    "                for x, y in np.float32(feature_points).reshape(-1, 2):\n",
    "                    tracking_paths.append([(x, y)])\n",
    "                    \n",
    "        #Update variables\n",
    "        frame_index += 1\n",
    "        prev_gray = frame_gray\n",
    "        \n",
    "        #Display output\n",
    "        cv2.imshow('Optical Flow', output_img)\n",
    "        \n",
    "        #Check if the user hits the 'Esc' key to exit\n",
    "        stop_key = cv2.waitKey(1)\n",
    "        if stop_key == 27:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tracking()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Haar cascades for object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Haar cascade file corresponding to face detection\n",
    "face_cascade = cv2.CascadeClassifier('data/haar_cascade_files/haarcascade_frontalface_default.xml')\n",
    "\n",
    "#Check if the file has been loaded correctly\n",
    "if face_cascade.empty():\n",
    "    raise IOError('Unable to load the face cascade classifier xml file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the video capture object\n",
    "cap = cv2.VideoCapture('data/rodney_darkoo_dance.mp4')\n",
    "\n",
    "#Define the scaling factor\n",
    "scaling_factor = 0.5\n",
    "\n",
    "#Iterate until the user hits the 'Esc' key\n",
    "while True:\n",
    "    #Capture the current frame\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    #Resize the frame\n",
    "    frame = cv2.resize(frame, None, fx = scaling_factor, fy = scaling_factor, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    #Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Run the face detector on the grayscale image\n",
    "    face_rects = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    #Draw a rectangle around the captured face\n",
    "    for (x, y, w, h) in face_rects:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "        \n",
    "    #Display the output\n",
    "    cv2.imshow('Face Detector', frame)\n",
    "    \n",
    "    #Check if User hits the 'Esc' key\n",
    "    stop_key = cv2.waitKey(1)\n",
    "    if stop_key == 27:\n",
    "        break\n",
    "        \n",
    "#Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "#Close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eye detection and tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Haar cascade files for face and eye\n",
    "face_cascade = cv2.CascadeClassifier('data/haar_cascade_files/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('data/haar_cascade_files/haarcascade_eye.xml')\n",
    "\n",
    "#Check if the files were loaded correctly\n",
    "if face_cascade.empty():\n",
    "    raise IOError('Unable to load the face cascade classifier xml file')\n",
    "elif eye_cascade.empty():\n",
    "    raise IOError('Unable to load the eye cascade classifier xml file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the video capture object\n",
    "cap = cv2.VideoCapture('data/charlies_angels.mp4')\n",
    "\n",
    "#Define the scaling factor\n",
    "scaling_factor = 0.5\n",
    "\n",
    "#Iterate until User hits the 'Esc' key\n",
    "while True:\n",
    "    #Capture the current frame\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    #Resize the frame\n",
    "    frame = cv2.resize(frame, None, fx = scaling_factor, fy = scaling_factor, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    #Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Run the face detector on the grayscale image\n",
    "    faces_rect = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    #For each face that's detected, run the eye detector\n",
    "    for (x, y, w, h) in faces_rect:\n",
    "        #Extract the grayscale face region of interest (ROI)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        \n",
    "        #Extract the color face ROI for future use\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        #Run the eye detector on the grayscale ROI\n",
    "        eyes_rect = eye_cascade.detectMultiScale(roi_gray)\n",
    "        \n",
    "        #Draw circles around the found eyes\n",
    "        for (x_eye, y_eye, w_eye, h_eye) in eyes_rect:\n",
    "            center = (int(x_eye + 0.5 * w_eye), int(y_eye + 0.5 * h_eye))\n",
    "            radius = int(0.3 * (w_eye + h_eye))\n",
    "            \n",
    "            color = (0, 255, 0)\n",
    "            thickness = 3\n",
    "            \n",
    "            cv2.circle(roi_color, center, radius, color, thickness)\n",
    "            \n",
    "    #Display the output\n",
    "    cv2.imshow('Eye detector', frame)\n",
    "    \n",
    "    #Check if the user hits the 'Esc' key\n",
    "    stop_key = cv2.waitKey(1)\n",
    "    if stop_key == 27:\n",
    "        break\n",
    "        \n",
    "#Release the videocapture object\n",
    "cap.release()\n",
    "\n",
    "#Close all the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
